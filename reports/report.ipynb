{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import defaultdict\n",
    "from itertools import chain, product\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = Path('../data/prepared_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(fpath, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>mccs</th>\n",
       "      <th>mccs_count</th>\n",
       "      <th>transaction_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001f322716470bf9bfc1708f06f00fc</td>\n",
       "      <td>6011,6011,6011,6011,6011,6011,6011</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001f322716470bf9bfc1708f06f00fc</td>\n",
       "      <td>6011,6011,6011,6011,6011,6011,6011</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001f322716470bf9bfc1708f06f00fc</td>\n",
       "      <td>5541,6011,6011,6011,6011,6011,6011,6011,6011</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001f322716470bf9bfc1708f06f00fc</td>\n",
       "      <td>5411,5411,5541,5912,6011,6011,6011,6011,6011</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001f322716470bf9bfc1708f06f00fc</td>\n",
       "      <td>5411,5411,5411,5411,5499,5541,5541,5999,5999,6...</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001f322716470bf9bfc1708f06f00fc</td>\n",
       "      <td>5411,5411,5541,5999,6011,6011,6011</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0001f322716470bf9bfc1708f06f00fc</td>\n",
       "      <td>5411,5411,6011,6011,6011,6011,6011</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0001f322716470bf9bfc1708f06f00fc</td>\n",
       "      <td>5211,5411,5541,6011,6011,6011,6011,6011,6011,6...</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0007297d86e14bd68bd87b1dbdefe302</td>\n",
       "      <td>6011,6011,6011,6011,6011,6011</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0007297d86e14bd68bd87b1dbdefe302</td>\n",
       "      <td>6011,6011,6011,6011,6011,6011</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        customer_id  \\\n",
       "0  0001f322716470bf9bfc1708f06f00fc   \n",
       "1  0001f322716470bf9bfc1708f06f00fc   \n",
       "2  0001f322716470bf9bfc1708f06f00fc   \n",
       "3  0001f322716470bf9bfc1708f06f00fc   \n",
       "4  0001f322716470bf9bfc1708f06f00fc   \n",
       "5  0001f322716470bf9bfc1708f06f00fc   \n",
       "6  0001f322716470bf9bfc1708f06f00fc   \n",
       "7  0001f322716470bf9bfc1708f06f00fc   \n",
       "8  0007297d86e14bd68bd87b1dbdefe302   \n",
       "9  0007297d86e14bd68bd87b1dbdefe302   \n",
       "\n",
       "                                                mccs  mccs_count  \\\n",
       "0                 6011,6011,6011,6011,6011,6011,6011           7   \n",
       "1                 6011,6011,6011,6011,6011,6011,6011           7   \n",
       "2       5541,6011,6011,6011,6011,6011,6011,6011,6011           9   \n",
       "3       5411,5411,5541,5912,6011,6011,6011,6011,6011           9   \n",
       "4  5411,5411,5411,5411,5499,5541,5541,5999,5999,6...          18   \n",
       "5                 5411,5411,5541,5999,6011,6011,6011           7   \n",
       "6                 5411,5411,6011,6011,6011,6011,6011           7   \n",
       "7  5211,5411,5541,6011,6011,6011,6011,6011,6011,6...          11   \n",
       "8                      6011,6011,6011,6011,6011,6011           6   \n",
       "9                      6011,6011,6011,6011,6011,6011           6   \n",
       "\n",
       "   transaction_month  \n",
       "0                  2  \n",
       "1                  3  \n",
       "2                  4  \n",
       "3                  5  \n",
       "4                  6  \n",
       "5                  7  \n",
       "6                  8  \n",
       "7                  9  \n",
       "8                  2  \n",
       "9                  3  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В столбце `mccs` в `data` записан список всех категорий товаров, приобретённых в данном месяце, разделённых запятой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_int_list(list_string):\n",
    "    return [int(item) for item in list_string.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "baskets = [\n",
    "    deserialize_int_list(list_string)\n",
    "    for list_string in data['mccs']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items = list(chain.from_iterable(baskets))\n",
    "\n",
    "unique_items = sorted(list(set(all_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_customers = data['customer_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем векторы корзин с помощью Bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_basket(basket, vocabulary):\n",
    "    vector = np.zeros(len(vocabulary))\n",
    "    \n",
    "    for item in basket:\n",
    "        item_id = vocabulary.index(item)\n",
    "        \n",
    "        vector[item_id] += 1\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_baskets = np.array([encode_basket(basket, unique_items) for basket in baskets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Масштабируем значения в векторах:\n",
    "\n",
    "* логарифмируем\n",
    "* располагаем между 0 и 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_baskets = np.log2(encoded_baskets + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "encoded_baskets = scaler.fit_transform(encoded_baskets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующая функция получает на вход длину сиквенса `sequence_length` и возвращает обучающую и тестовую выборки для данной длины. Для каждого пользователя:\n",
    "\n",
    "1. собираем все корзины.\n",
    "2. получаем из них всевозможные последовательности корзин длины `sequence_length`, причём, если в качестве `x` мы берём слайс `baskets[i:j]`, то в качестве соответствующего `y` выступит `baskets[i+1 : j+1]`.\n",
    "3. все `y` бинаризируем по правилу `y = (y > 0).astype(int)`.\n",
    "4. все такие последовательности кроме последней (для которой `y` содержит самую последнюю корзину) складываем в обучающую выборку, а последнюю - в тестовую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9988/9988 [00:25<00:00, 397.87it/s]\n"
     ]
    }
   ],
   "source": [
    "customer2baskets = {\n",
    "    customer_id: encoded_baskets[data['customer_id'] == customer_id]\n",
    "    for customer_id in tqdm(unique_customers)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test_data(sequence_length=3):\n",
    "    x_train = list()\n",
    "    y_train = list()\n",
    "\n",
    "    x_test = list()\n",
    "    y_test = list()\n",
    "\n",
    "    for customer_id in unique_customers:\n",
    "        customer_baskets = customer2baskets[customer_id]\n",
    "        baskets_count = len(customer_baskets)\n",
    "\n",
    "        sequence_count = baskets_count - sequence_length\n",
    "\n",
    "        if sequence_count < 2:\n",
    "            continue\n",
    "\n",
    "        for start_id in range(sequence_count):\n",
    "            end_id = start_id + sequence_length\n",
    "\n",
    "            x = customer_baskets[start_id : end_id].copy()\n",
    "            y = customer_baskets[start_id + 1 : end_id + 1].copy()\n",
    "            y = (y > 0).astype(int)\n",
    "\n",
    "            if start_id < sequence_count - 1:\n",
    "                x_train.append(x)\n",
    "                y_train.append(y)\n",
    "            else:\n",
    "                x_test.append(x)\n",
    "                y_test.append(y)\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    x_test = np.array(x_test)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем последовательности всевозможной длины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = dict()\n",
    "y_train = dict()\n",
    "\n",
    "x_test = dict()\n",
    "y_test = dict()\n",
    "\n",
    "for sequence_length in range(1, 13):\n",
    "    x_train_, x_test_, y_train_, y_test_ = get_train_and_test_data(sequence_length)\n",
    "    \n",
    "    if len(x_train_) == 0:\n",
    "        break\n",
    "    \n",
    "    x_train[sequence_length] = x_train_\n",
    "    y_train[sequence_length] = y_train_\n",
    "    \n",
    "    x_test[sequence_length] = x_test_\n",
    "    y_test[sequence_length] = y_test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(\n",
    "        512,\n",
    "        input_shape=(None, len(unique_items)),\n",
    "        return_sequences=True,\n",
    "    ),\n",
    "    tf.keras.layers.Dense(\n",
    "        512,\n",
    "        activation='sigmoid',\n",
    "    ),\n",
    "    tf.keras.layers.Dense(\n",
    "        len(unique_items),\n",
    "        activation='sigmoid',\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последовательно обучаем модель на сиквенсах каждой длины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1.\n",
      "Train on 46501 samples, validate on 9617 samples\n",
      "46501/46501 [==============================] - 38s 817us/sample - loss: 0.0504 - val_loss: 0.0455\n",
      "Train on 36884 samples, validate on 9165 samples\n",
      "36884/36884 [==============================] - 40s 1ms/sample - loss: 0.0470 - val_loss: 0.0457\n",
      "Train on 27719 samples, validate on 8549 samples\n",
      "27719/27719 [==============================] - 39s 1ms/sample - loss: 0.0465 - val_loss: 0.0468\n",
      "Train on 19170 samples, validate on 7703 samples\n",
      "19170/19170 [==============================] - 34s 2ms/sample - loss: 0.0459 - val_loss: 0.0466\n",
      "Train on 11467 samples, validate on 6149 samples\n",
      "11467/11467 [==============================] - 25s 2ms/sample - loss: 0.0451 - val_loss: 0.0455\n",
      "Train on 5318 samples, validate on 4840 samples\n",
      "5318/5318 [==============================] - 14s 3ms/sample - loss: 0.0402 - val_loss: 0.0544\n",
      "Train on 478 samples, validate on 478 samples\n",
      "478/478 [==============================] - 1s 3ms/sample - loss: 0.0372 - val_loss: 0.0451\n",
      "\n",
      "Epoch 2.\n",
      "Train on 46501 samples, validate on 9617 samples\n",
      "46501/46501 [==============================] - 36s 767us/sample - loss: 0.0448 - val_loss: 0.0442\n",
      "Train on 36884 samples, validate on 9165 samples\n",
      "36884/36884 [==============================] - 39s 1ms/sample - loss: 0.0455 - val_loss: 0.0449\n",
      "Train on 27719 samples, validate on 8549 samples\n",
      "27719/27719 [==============================] - 39s 1ms/sample - loss: 0.0450 - val_loss: 0.0452\n",
      "Train on 19170 samples, validate on 7703 samples\n",
      "19170/19170 [==============================] - 35s 2ms/sample - loss: 0.0442 - val_loss: 0.0455\n",
      "Train on 11467 samples, validate on 6149 samples\n",
      "11467/11467 [==============================] - 25s 2ms/sample - loss: 0.0433 - val_loss: 0.0435\n",
      "Train on 5318 samples, validate on 4840 samples\n",
      "5318/5318 [==============================] - 14s 3ms/sample - loss: 0.0382 - val_loss: 0.0533\n",
      "Train on 478 samples, validate on 478 samples\n",
      "478/478 [==============================] - 1s 3ms/sample - loss: 0.0349 - val_loss: 0.0427\n",
      "\n",
      "Epoch 3.\n",
      "Train on 46501 samples, validate on 9617 samples\n",
      "46501/46501 [==============================] - 38s 813us/sample - loss: 0.0442 - val_loss: 0.0440\n",
      "Train on 36884 samples, validate on 9165 samples\n",
      "36884/36884 [==============================] - 42s 1ms/sample - loss: 0.0446 - val_loss: 0.0447\n",
      "Train on 27719 samples, validate on 8549 samples\n",
      "27719/27719 [==============================] - 39s 1ms/sample - loss: 0.0438 - val_loss: 0.0449\n",
      "Train on 19170 samples, validate on 7703 samples\n",
      "19170/19170 [==============================] - 33s 2ms/sample - loss: 0.0425 - val_loss: 0.0444\n",
      "Train on 11467 samples, validate on 6149 samples\n",
      "11467/11467 [==============================] - 24s 2ms/sample - loss: 0.0411 - val_loss: 0.0412\n",
      "Train on 5318 samples, validate on 4840 samples\n",
      "5318/5318 [==============================] - 14s 3ms/sample - loss: 0.0357 - val_loss: 0.0509\n",
      "Train on 478 samples, validate on 478 samples\n",
      "478/478 [==============================] - 1s 3ms/sample - loss: 0.0319 - val_loss: 0.0399\n",
      "\n",
      "Epoch 4.\n",
      "Train on 46501 samples, validate on 9617 samples\n",
      "46501/46501 [==============================] - 37s 785us/sample - loss: 0.0437 - val_loss: 0.0440\n",
      "Train on 36884 samples, validate on 9165 samples\n",
      "36884/36884 [==============================] - 41s 1ms/sample - loss: 0.0438 - val_loss: 0.0445\n",
      "Train on 27719 samples, validate on 8549 samples\n",
      "27719/27719 [==============================] - 41s 1ms/sample - loss: 0.0423 - val_loss: 0.0446\n",
      "Train on 19170 samples, validate on 7703 samples\n",
      "19170/19170 [==============================] - 33s 2ms/sample - loss: 0.0400 - val_loss: 0.0436\n",
      "Train on 11467 samples, validate on 6149 samples\n",
      "11467/11467 [==============================] - 24s 2ms/sample - loss: 0.0382 - val_loss: 0.0383\n",
      "Train on 5318 samples, validate on 4840 samples\n",
      "5318/5318 [==============================] - 14s 3ms/sample - loss: 0.0322 - val_loss: 0.0481\n",
      "Train on 478 samples, validate on 478 samples\n",
      "478/478 [==============================] - 1s 3ms/sample - loss: 0.0281 - val_loss: 0.0362\n",
      "\n",
      "Epoch 5.\n",
      "Train on 46501 samples, validate on 9617 samples\n",
      "46501/46501 [==============================] - 36s 784us/sample - loss: 0.0432 - val_loss: 0.0443\n",
      "Train on 36884 samples, validate on 9165 samples\n",
      "36884/36884 [==============================] - 40s 1ms/sample - loss: 0.0428 - val_loss: 0.0449\n",
      "Train on 27719 samples, validate on 8549 samples\n",
      "27719/27719 [==============================] - 39s 1ms/sample - loss: 0.0404 - val_loss: 0.0449\n",
      "Train on 19170 samples, validate on 7703 samples\n",
      "19170/19170 [==============================] - 33s 2ms/sample - loss: 0.0370 - val_loss: 0.0432\n",
      "Train on 11467 samples, validate on 6149 samples\n",
      "11467/11467 [==============================] - 24s 2ms/sample - loss: 0.0346 - val_loss: 0.0355\n",
      "Train on 5318 samples, validate on 4840 samples\n",
      "5318/5318 [==============================] - 14s 3ms/sample - loss: 0.0286 - val_loss: 0.0453\n",
      "Train on 478 samples, validate on 478 samples\n",
      "478/478 [==============================] - 1s 3ms/sample - loss: 0.0244 - val_loss: 0.0336\n",
      "\n",
      "Epoch 6.\n",
      "Train on 46501 samples, validate on 9617 samples\n",
      "46501/46501 [==============================] - 36s 768us/sample - loss: 0.0428 - val_loss: 0.0448\n",
      "Train on 36884 samples, validate on 9165 samples\n",
      "36884/36884 [==============================] - 40s 1ms/sample - loss: 0.0417 - val_loss: 0.0456\n",
      "Train on 27719 samples, validate on 8549 samples\n",
      "27719/27719 [==============================] - 38s 1ms/sample - loss: 0.0383 - val_loss: 0.0457\n",
      "Train on 19170 samples, validate on 7703 samples\n",
      "19170/19170 [==============================] - 33s 2ms/sample - loss: 0.0338 - val_loss: 0.0430\n",
      "Train on 11467 samples, validate on 6149 samples\n",
      "11467/11467 [==============================] - 24s 2ms/sample - loss: 0.0314 - val_loss: 0.0333\n",
      "Train on 5318 samples, validate on 4840 samples\n",
      "5318/5318 [==============================] - 14s 3ms/sample - loss: 0.0253 - val_loss: 0.0439\n",
      "Train on 478 samples, validate on 478 samples\n",
      "478/478 [==============================] - 1s 3ms/sample - loss: 0.0213 - val_loss: 0.0321\n",
      "\n",
      "Epoch 7.\n",
      "Train on 46501 samples, validate on 9617 samples\n",
      "46501/46501 [==============================] - 36s 766us/sample - loss: 0.0423 - val_loss: 0.0454\n",
      "Train on 36884 samples, validate on 9165 samples\n",
      "36884/36884 [==============================] - 40s 1ms/sample - loss: 0.0406 - val_loss: 0.0465\n",
      "Train on 27719 samples, validate on 8549 samples\n",
      "27719/27719 [==============================] - 39s 1ms/sample - loss: 0.0362 - val_loss: 0.0464\n",
      "Train on 19170 samples, validate on 7703 samples\n",
      "19170/19170 [==============================] - 33s 2ms/sample - loss: 0.0310 - val_loss: 0.0429\n",
      "Train on 11467 samples, validate on 6149 samples\n",
      "11467/11467 [==============================] - 25s 2ms/sample - loss: 0.0287 - val_loss: 0.0320\n",
      "Train on 5318 samples, validate on 4840 samples\n",
      "5318/5318 [==============================] - 15s 3ms/sample - loss: 0.0228 - val_loss: 0.0432\n",
      "Train on 478 samples, validate on 478 samples\n",
      "478/478 [==============================] - 1s 3ms/sample - loss: 0.0193 - val_loss: 0.0311\n",
      "\n",
      "Epoch 8.\n",
      "Train on 46501 samples, validate on 9617 samples\n",
      "46501/46501 [==============================] - 36s 773us/sample - loss: 0.0419 - val_loss: 0.0460\n",
      "Train on 36884 samples, validate on 9165 samples\n",
      "36884/36884 [==============================] - 40s 1ms/sample - loss: 0.0395 - val_loss: 0.0476\n",
      "Train on 27719 samples, validate on 8549 samples\n",
      "27719/27719 [==============================] - 39s 1ms/sample - loss: 0.0343 - val_loss: 0.0474\n",
      "Train on 19170 samples, validate on 7703 samples\n",
      "19170/19170 [==============================] - 33s 2ms/sample - loss: 0.0287 - val_loss: 0.0429\n",
      "Train on 11467 samples, validate on 6149 samples\n",
      "11467/11467 [==============================] - 24s 2ms/sample - loss: 0.0267 - val_loss: 0.0313\n",
      "Train on 5318 samples, validate on 4840 samples\n",
      "5318/5318 [==============================] - 14s 3ms/sample - loss: 0.0211 - val_loss: 0.0428\n",
      "Train on 478 samples, validate on 478 samples\n",
      "478/478 [==============================] - 1s 3ms/sample - loss: 0.0178 - val_loss: 0.0309\n",
      "\n",
      "Epoch 9.\n",
      "Train on 46501 samples, validate on 9617 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46501/46501 [==============================] - 36s 772us/sample - loss: 0.0414 - val_loss: 0.0468\n",
      "Train on 36884 samples, validate on 9165 samples\n",
      "36884/36884 [==============================] - 41s 1ms/sample - loss: 0.0384 - val_loss: 0.0488\n",
      "Train on 27719 samples, validate on 8549 samples\n",
      "27719/27719 [==============================] - 38s 1ms/sample - loss: 0.0325 - val_loss: 0.0484\n",
      "Train on 19170 samples, validate on 7703 samples\n",
      "19170/19170 [==============================] - 33s 2ms/sample - loss: 0.0269 - val_loss: 0.0429\n",
      "Train on 11467 samples, validate on 6149 samples\n",
      "11467/11467 [==============================] - 24s 2ms/sample - loss: 0.0251 - val_loss: 0.0311\n",
      "Train on 5318 samples, validate on 4840 samples\n",
      "5318/5318 [==============================] - 14s 3ms/sample - loss: 0.0197 - val_loss: 0.0431\n",
      "Train on 478 samples, validate on 478 samples\n",
      "478/478 [==============================] - 1s 3ms/sample - loss: 0.0170 - val_loss: 0.0311\n",
      "\n",
      "Epoch 10.\n",
      "Train on 46501 samples, validate on 9617 samples\n",
      "46501/46501 [==============================] - 37s 785us/sample - loss: 0.0409 - val_loss: 0.0474\n",
      "Train on 36884 samples, validate on 9165 samples\n",
      "36884/36884 [==============================] - 41s 1ms/sample - loss: 0.0373 - val_loss: 0.0500\n",
      "Train on 27719 samples, validate on 8549 samples\n",
      "27719/27719 [==============================] - 39s 1ms/sample - loss: 0.0310 - val_loss: 0.0492\n",
      "Train on 19170 samples, validate on 7703 samples\n",
      "19170/19170 [==============================] - 33s 2ms/sample - loss: 0.0254 - val_loss: 0.0429\n",
      "Train on 11467 samples, validate on 6149 samples\n",
      "11467/11467 [==============================] - 24s 2ms/sample - loss: 0.0239 - val_loss: 0.0313\n",
      "Train on 5318 samples, validate on 4840 samples\n",
      "5318/5318 [==============================] - 15s 3ms/sample - loss: 0.0187 - val_loss: 0.0432\n",
      "Train on 478 samples, validate on 478 samples\n",
      "478/478 [==============================] - 1s 3ms/sample - loss: 0.0161 - val_loss: 0.0313\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}.')\n",
    "    \n",
    "    for sequence_length in x_train:\n",
    "        model.fit(\n",
    "            x_train[sequence_length],\n",
    "            y_train[sequence_length],\n",
    "            verbose=1,\n",
    "            validation_data=(x_test[sequence_length], y_test[sequence_length]),\n",
    "        )\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_test, y_pred):\n",
    "    f1s = list()\n",
    "    \n",
    "    for test, pred in zip(y_test, y_pred):\n",
    "        test = test[-1]\n",
    "        pred = pred[-1]\n",
    "        \n",
    "        f1_ = f1_score(test, (pred > 0.5).astype(int))\n",
    "        \n",
    "        f1s.append(f1_)\n",
    "    \n",
    "    return np.mean(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len\tf1\n",
      "1\t0.5161623532610122\n",
      "2\t0.547015217037395\n",
      "3\t0.5521767385920417\n",
      "4\t0.5515519010051602\n",
      "5\t0.5683910411441858\n",
      "6\t0.5851163189989425\n",
      "7\t0.5915963292597508\n"
     ]
    }
   ],
   "source": [
    "print('seq_len\\tf1')\n",
    "\n",
    "for sequence_length in x_test:\n",
    "    y_pred = model.predict(x_test[sequence_length])\n",
    "    \n",
    "    print(f'{sequence_length}\\t{f1(y_test[sequence_length], y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Топ-5 предсказанных категорий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlargest_ids(array, top=5):\n",
    "    return array.argsort()[:-top - 1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_items = np.array(unique_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer id: 9c228cd3cbeb00c93ed885ae7e197e58\n",
      "real: [4814 5541 5921 5999 6011]\n",
      "pred: [5331 5411 5499 5921 6011]\n",
      "\n",
      "customer id: 8962a7c28f574f50c2b9ffb00a0f55ec\n",
      "real: [5411 5541 5812 5814 6011]\n",
      "pred: [5411 5541 5812 5814 6011]\n",
      "\n",
      "customer id: 38bf8ee18ffcc950a9d43fe31fe0bb20\n",
      "real: [5072 5074 5311 5411 6011]\n",
      "pred: [5311 5411 5651 5921 6011]\n",
      "\n",
      "customer id: f20fd2d988da3846af01432ecd5b24b1\n",
      "real: [5411 5691 5699 5814 7230]\n",
      "pred: [5411 5541 5812 5814 6011]\n",
      "\n",
      "customer id: b6dd3b2f0866b2e4fd97ffe5a26a4928\n",
      "real: [5499 5511 5651 5912 6011]\n",
      "pred: [5411 5541 5814 5912 6011]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for customer_id in np.random.choice(unique_customers, 5, replace=False):\n",
    "    customer_baskets = encoded_baskets[data['customer_id'] == customer_id]\n",
    "\n",
    "    next_basket_real = customer_baskets[-1]\n",
    "\n",
    "    top_5_ids_real = get_nlargest_ids(next_basket_real)\n",
    "    top_5_results_real = unique_items[top_5_ids_real]\n",
    "\n",
    "    customer_baskets = customer_baskets[:-1]\n",
    "    customer_baskets = customer_baskets[np.newaxis, :]\n",
    "    \n",
    "    next_baskets_pred = model.predict(customer_baskets)[0]\n",
    "    next_basket_pred = next_baskets_pred[-1]\n",
    "\n",
    "    top_5_ids_pred = get_nlargest_ids(next_basket_pred)\n",
    "    top_5_results_pred = unique_items[top_5_ids_pred]\n",
    "    \n",
    "    top_5_results_real.sort()\n",
    "    top_5_results_pred.sort()\n",
    "    \n",
    "    print(f'customer id: {customer_id}')\n",
    "    \n",
    "    print(f'real: {top_5_results_real}')\n",
    "    print(f'pred: {top_5_results_pred}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
