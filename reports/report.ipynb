{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import defaultdict\n",
    "from itertools import chain, product\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = Path('../data/prepared_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(fpath, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>mccs</th>\n",
       "      <th>mccs_count</th>\n",
       "      <th>transaction_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001f322716470bf9bfc1708f06f00fc</td>\n",
       "      <td>6011,6011,6011,6011,6011,6011,6011</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001f322716470bf9bfc1708f06f00fc</td>\n",
       "      <td>6011,6011,6011,6011,6011,6011,6011</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001f322716470bf9bfc1708f06f00fc</td>\n",
       "      <td>5541,6011,6011,6011,6011,6011,6011,6011,6011</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001f322716470bf9bfc1708f06f00fc</td>\n",
       "      <td>5411,5411,5541,5912,6011,6011,6011,6011,6011</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001f322716470bf9bfc1708f06f00fc</td>\n",
       "      <td>5411,5411,5411,5411,5499,5541,5541,5999,5999,6...</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001f322716470bf9bfc1708f06f00fc</td>\n",
       "      <td>5411,5411,5541,5999,6011,6011,6011</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0001f322716470bf9bfc1708f06f00fc</td>\n",
       "      <td>5411,5411,6011,6011,6011,6011,6011</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0001f322716470bf9bfc1708f06f00fc</td>\n",
       "      <td>5211,5411,5541,6011,6011,6011,6011,6011,6011,6...</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0007297d86e14bd68bd87b1dbdefe302</td>\n",
       "      <td>6011,6011,6011,6011,6011,6011</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0007297d86e14bd68bd87b1dbdefe302</td>\n",
       "      <td>6011,6011,6011,6011,6011,6011</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        customer_id  \\\n",
       "0  0001f322716470bf9bfc1708f06f00fc   \n",
       "1  0001f322716470bf9bfc1708f06f00fc   \n",
       "2  0001f322716470bf9bfc1708f06f00fc   \n",
       "3  0001f322716470bf9bfc1708f06f00fc   \n",
       "4  0001f322716470bf9bfc1708f06f00fc   \n",
       "5  0001f322716470bf9bfc1708f06f00fc   \n",
       "6  0001f322716470bf9bfc1708f06f00fc   \n",
       "7  0001f322716470bf9bfc1708f06f00fc   \n",
       "8  0007297d86e14bd68bd87b1dbdefe302   \n",
       "9  0007297d86e14bd68bd87b1dbdefe302   \n",
       "\n",
       "                                                mccs  mccs_count  \\\n",
       "0                 6011,6011,6011,6011,6011,6011,6011           7   \n",
       "1                 6011,6011,6011,6011,6011,6011,6011           7   \n",
       "2       5541,6011,6011,6011,6011,6011,6011,6011,6011           9   \n",
       "3       5411,5411,5541,5912,6011,6011,6011,6011,6011           9   \n",
       "4  5411,5411,5411,5411,5499,5541,5541,5999,5999,6...          18   \n",
       "5                 5411,5411,5541,5999,6011,6011,6011           7   \n",
       "6                 5411,5411,6011,6011,6011,6011,6011           7   \n",
       "7  5211,5411,5541,6011,6011,6011,6011,6011,6011,6...          11   \n",
       "8                      6011,6011,6011,6011,6011,6011           6   \n",
       "9                      6011,6011,6011,6011,6011,6011           6   \n",
       "\n",
       "   transaction_month  \n",
       "0                  2  \n",
       "1                  3  \n",
       "2                  4  \n",
       "3                  5  \n",
       "4                  6  \n",
       "5                  7  \n",
       "6                  8  \n",
       "7                  9  \n",
       "8                  2  \n",
       "9                  3  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В столбце `mccs` в `data` записан список всех категорий товаров, приобретённых в данном месяце, разделённых запятой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_int_list(list_string):\n",
    "    return [int(item) for item in list_string.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "baskets = [\n",
    "    deserialize_int_list(list_string)\n",
    "    for list_string in data['mccs']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items = list(chain.from_iterable(baskets))\n",
    "\n",
    "unique_items = sorted(list(set(all_items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_customers = data['customer_id'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем векторы корзин с помощью Bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_basket(basket, vocabulary):\n",
    "    vector = np.zeros(len(vocabulary))\n",
    "    \n",
    "    for item in basket:\n",
    "        item_id = vocabulary.index(item)\n",
    "        \n",
    "        vector[item_id] += 1\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_baskets = np.array([encode_basket(basket, unique_items) for basket in baskets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Масштабируем значения в векторах:\n",
    "\n",
    "* логарифмируем\n",
    "* располагаем между 0 и 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_baskets = np.log2(encoded_baskets + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "encoded_baskets = scaler.fit_transform(encoded_baskets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующая функция получает на вход длину сиквенса `sequence_length` и возвращает обучающую и тестовую выборки для данной длины. Для каждого пользователя:\n",
    "\n",
    "1. собираем все корзины.\n",
    "2. получаем из них всевозможные последовательности корзин длины `sequence_length`, причём, если в качестве `x` мы берём слайс `baskets[i:j]`, то в качестве соответствующего `y` выступит `baskets[i+1 : j+1]`.\n",
    "3. все `y` бинаризируем по правилу `y = (y > 0).astype(int)`.\n",
    "4. все такие последовательности кроме последней (для которой `y` содержит самую последнюю корзину) складываем в обучающую выборку, а последнюю - в тестовую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9988/9988 [00:22<00:00, 443.10it/s]\n"
     ]
    }
   ],
   "source": [
    "customer2baskets = {\n",
    "    customer_id: encoded_baskets[data['customer_id'] == customer_id]\n",
    "    for customer_id in tqdm(unique_customers)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test_data(sequence_length=3):\n",
    "    x_train = list()\n",
    "    y_train = list()\n",
    "\n",
    "    x_test = list()\n",
    "    y_test = list()\n",
    "\n",
    "    for customer_id in unique_customers:\n",
    "        customer_baskets = customer2baskets[customer_id]\n",
    "        baskets_count = len(customer_baskets)\n",
    "\n",
    "        sequence_count = baskets_count - sequence_length\n",
    "\n",
    "        if sequence_count < 2:\n",
    "            continue\n",
    "\n",
    "        for start_id in range(sequence_count):\n",
    "            end_id = start_id + sequence_length\n",
    "\n",
    "            x = customer_baskets[start_id : end_id].copy()\n",
    "            y = customer_baskets[start_id + 1 : end_id + 1].copy()\n",
    "            y = (y > 0).astype(int)\n",
    "\n",
    "            if start_id < sequence_count - 1:\n",
    "                x_train.append(x)\n",
    "                y_train.append(y)\n",
    "            else:\n",
    "                x_test.append(x)\n",
    "                y_test.append(y)\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    x_test = np.array(x_test)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем последовательности всевозможной длины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = dict()\n",
    "y_train = dict()\n",
    "\n",
    "x_test = dict()\n",
    "y_test = dict()\n",
    "\n",
    "for sequence_length in range(1, 13):\n",
    "    x_train_, x_test_, y_train_, y_test_ = get_train_and_test_data(sequence_length)\n",
    "    \n",
    "    if len(x_train_) == 0:\n",
    "        break\n",
    "    \n",
    "    x_train[sequence_length] = x_train_\n",
    "    y_train[sequence_length] = y_train_\n",
    "    \n",
    "    x_test[sequence_length] = x_test_\n",
    "    y_test[sequence_length] = y_test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(\n",
    "        512,\n",
    "        input_shape=(None, len(unique_items)),\n",
    "        return_sequences=True,\n",
    "        dropout=0.2,\n",
    "        recurrent_dropout=0.2,\n",
    "    ),\n",
    "    tf.keras.layers.Dense(\n",
    "        512,\n",
    "        activation='sigmoid',\n",
    "    ),\n",
    "    tf.keras.layers.Dense(\n",
    "        len(unique_items),\n",
    "        activation='sigmoid',\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последовательно обучаем модель на сиквенсах каждой длины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1.\n",
      "Train on 46501 samples, validate on 9617 samples\n",
      "46501/46501 [==============================] - 36s 781us/sample - loss: 0.0511 - val_loss: 0.0458\n",
      "Train on 36884 samples, validate on 9165 samples\n",
      "36884/36884 [==============================] - 40s 1ms/sample - loss: 0.0482 - val_loss: 0.0460\n",
      "Train on 27719 samples, validate on 8549 samples\n",
      "27719/27719 [==============================] - 39s 1ms/sample - loss: 0.0477 - val_loss: 0.0468\n",
      "Train on 19170 samples, validate on 7703 samples\n",
      "19170/19170 [==============================] - 34s 2ms/sample - loss: 0.0472 - val_loss: 0.0470\n",
      "Train on 11467 samples, validate on 6149 samples\n",
      "11467/11467 [==============================] - 25s 2ms/sample - loss: 0.0464 - val_loss: 0.0462\n",
      "Train on 5318 samples, validate on 4840 samples\n",
      "5318/5318 [==============================] - 14s 3ms/sample - loss: 0.0415 - val_loss: 0.0555\n",
      "Train on 478 samples, validate on 478 samples\n",
      "478/478 [==============================] - 1s 3ms/sample - loss: 0.0392 - val_loss: 0.0478\n",
      "\n",
      "Epoch 2.\n",
      "Train on 46501 samples, validate on 9617 samples\n",
      "46501/46501 [==============================] - 36s 783us/sample - loss: 0.0458 - val_loss: 0.0443\n",
      "Train on 36884 samples, validate on 9165 samples\n",
      "36884/36884 [==============================] - 40s 1ms/sample - loss: 0.0465 - val_loss: 0.0450\n",
      "Train on 27719 samples, validate on 8549 samples\n",
      "27719/27719 [==============================] - 39s 1ms/sample - loss: 0.0463 - val_loss: 0.0454\n",
      "Train on 19170 samples, validate on 7703 samples\n",
      "19170/19170 [==============================] - 34s 2ms/sample - loss: 0.0459 - val_loss: 0.0459\n",
      "Train on 11467 samples, validate on 6149 samples\n",
      "11467/11467 [==============================] - 25s 2ms/sample - loss: 0.0452 - val_loss: 0.0448\n",
      "Train on 5318 samples, validate on 4840 samples\n",
      "5318/5318 [==============================] - 14s 3ms/sample - loss: 0.0403 - val_loss: 0.0553\n",
      "Train on 478 samples, validate on 478 samples\n",
      "478/478 [==============================] - 1s 3ms/sample - loss: 0.0380 - val_loss: 0.0452\n",
      "\n",
      "Epoch 3.\n",
      "Train on 46501 samples, validate on 9617 samples\n",
      "46501/46501 [==============================] - 36s 770us/sample - loss: 0.0451 - val_loss: 0.0448\n",
      "Train on 36884 samples, validate on 9165 samples\n",
      "36884/36884 [==============================] - 40s 1ms/sample - loss: 0.0458 - val_loss: 0.0447\n",
      "Train on 27719 samples, validate on 8549 samples\n",
      "27719/27719 [==============================] - 39s 1ms/sample - loss: 0.0455 - val_loss: 0.0452\n",
      "Train on 19170 samples, validate on 7703 samples\n",
      "19170/19170 [==============================] - 34s 2ms/sample - loss: 0.0450 - val_loss: 0.0452\n",
      "Train on 11467 samples, validate on 6149 samples\n",
      "11467/11467 [==============================] - 25s 2ms/sample - loss: 0.0443 - val_loss: 0.0438\n",
      "Train on 5318 samples, validate on 4840 samples\n",
      "5318/5318 [==============================] - 14s 3ms/sample - loss: 0.0394 - val_loss: 0.0548\n",
      "Train on 478 samples, validate on 478 samples\n",
      "478/478 [==============================] - 1s 3ms/sample - loss: 0.0370 - val_loss: 0.0446\n",
      "\n",
      "Epoch 4.\n",
      "Train on 46501 samples, validate on 9617 samples\n",
      "46501/46501 [==============================] - 35s 763us/sample - loss: 0.0446 - val_loss: 0.0439\n",
      "Train on 36884 samples, validate on 9165 samples\n",
      "36884/36884 [==============================] - 40s 1ms/sample - loss: 0.0452 - val_loss: 0.0445\n",
      "Train on 27719 samples, validate on 8549 samples\n",
      "27719/27719 [==============================] - 38s 1ms/sample - loss: 0.0448 - val_loss: 0.0447\n",
      "Train on 19170 samples, validate on 7703 samples\n",
      "19170/19170 [==============================] - 34s 2ms/sample - loss: 0.0441 - val_loss: 0.0444\n",
      "Train on 11467 samples, validate on 6149 samples\n",
      "11467/11467 [==============================] - 25s 2ms/sample - loss: 0.0434 - val_loss: 0.0426\n",
      "Train on 5318 samples, validate on 4840 samples\n",
      "5318/5318 [==============================] - 14s 3ms/sample - loss: 0.0385 - val_loss: 0.0539\n",
      "Train on 478 samples, validate on 478 samples\n",
      "478/478 [==============================] - 2s 3ms/sample - loss: 0.0357 - val_loss: 0.0430\n",
      "\n",
      "Epoch 5.\n",
      "Train on 46501 samples, validate on 9617 samples\n",
      "46501/46501 [==============================] - 37s 804us/sample - loss: 0.0442 - val_loss: 0.0444\n",
      "Train on 36884 samples, validate on 9165 samples\n",
      "36884/36884 [==============================] - 42s 1ms/sample - loss: 0.0447 - val_loss: 0.0440\n",
      "Train on 27719 samples, validate on 8549 samples\n",
      "27719/27719 [==============================] - 40s 1ms/sample - loss: 0.0441 - val_loss: 0.0444\n",
      "Train on 19170 samples, validate on 7703 samples\n",
      "19170/19170 [==============================] - 36s 2ms/sample - loss: 0.0432 - val_loss: 0.0437\n",
      "Train on 11467 samples, validate on 6149 samples\n",
      "11467/11467 [==============================] - 25s 2ms/sample - loss: 0.0424 - val_loss: 0.0413\n",
      "Train on 5318 samples, validate on 4840 samples\n",
      "5318/5318 [==============================] - 14s 3ms/sample - loss: 0.0375 - val_loss: 0.0524\n",
      "Train on 478 samples, validate on 478 samples\n",
      "478/478 [==============================] - 1s 3ms/sample - loss: 0.0345 - val_loss: 0.0414\n",
      "\n",
      "Epoch 6.\n",
      "Train on 46501 samples, validate on 9617 samples\n",
      "46501/46501 [==============================] - 39s 830us/sample - loss: 0.0439 - val_loss: 0.0442\n",
      "Train on 36884 samples, validate on 9165 samples\n",
      "36884/36884 [==============================] - 41s 1ms/sample - loss: 0.0442 - val_loss: 0.0443\n",
      "Train on 27719 samples, validate on 8549 samples\n",
      "27719/27719 [==============================] - 40s 1ms/sample - loss: 0.0433 - val_loss: 0.0442\n",
      "Train on 19170 samples, validate on 7703 samples\n",
      "19170/19170 [==============================] - 36s 2ms/sample - loss: 0.0422 - val_loss: 0.0430\n",
      "Train on 11467 samples, validate on 6149 samples\n",
      "11467/11467 [==============================] - 26s 2ms/sample - loss: 0.0414 - val_loss: 0.0400\n",
      "Train on 5318 samples, validate on 4840 samples\n",
      "5318/5318 [==============================] - 14s 3ms/sample - loss: 0.0364 - val_loss: 0.0514\n",
      "Train on 478 samples, validate on 478 samples\n",
      "478/478 [==============================] - 1s 3ms/sample - loss: 0.0330 - val_loss: 0.0403\n",
      "\n",
      "Epoch 7.\n",
      "Train on 46501 samples, validate on 9617 samples\n",
      "46501/46501 [==============================] - 37s 797us/sample - loss: 0.0436 - val_loss: 0.0443\n",
      "Train on 36884 samples, validate on 9165 samples\n",
      "36884/36884 [==============================] - 42s 1ms/sample - loss: 0.0436 - val_loss: 0.0443\n",
      "Train on 27719 samples, validate on 8549 samples\n",
      "27719/27719 [==============================] - 40s 1ms/sample - loss: 0.0425 - val_loss: 0.0440\n",
      "Train on 19170 samples, validate on 7703 samples\n",
      "19170/19170 [==============================] - 38s 2ms/sample - loss: 0.0412 - val_loss: 0.0424\n",
      "Train on 11467 samples, validate on 6149 samples\n",
      "11467/11467 [==============================] - 26s 2ms/sample - loss: 0.0403 - val_loss: 0.0387\n",
      "Train on 5318 samples, validate on 4840 samples\n",
      "5318/5318 [==============================] - 15s 3ms/sample - loss: 0.0353 - val_loss: 0.0498\n",
      "Train on 478 samples, validate on 478 samples\n",
      "478/478 [==============================] - 1s 3ms/sample - loss: 0.0319 - val_loss: 0.0390\n",
      "\n",
      "Epoch 8.\n",
      "Train on 46501 samples, validate on 9617 samples\n",
      "46501/46501 [==============================] - 37s 801us/sample - loss: 0.0433 - val_loss: 0.0443\n",
      "Train on 36884 samples, validate on 9165 samples\n",
      "36884/36884 [==============================] - 42s 1ms/sample - loss: 0.0431 - val_loss: 0.0445\n",
      "Train on 27719 samples, validate on 8549 samples\n",
      "27719/27719 [==============================] - 41s 1ms/sample - loss: 0.0417 - val_loss: 0.0438\n",
      "Train on 19170 samples, validate on 7703 samples\n",
      "19170/19170 [==============================] - 33s 2ms/sample - loss: 0.0402 - val_loss: 0.0419\n",
      "Train on 11467 samples, validate on 6149 samples\n",
      "11467/11467 [==============================] - 25s 2ms/sample - loss: 0.0392 - val_loss: 0.0373\n",
      "Train on 5318 samples, validate on 4840 samples\n",
      "5318/5318 [==============================] - 14s 3ms/sample - loss: 0.0342 - val_loss: 0.0486\n",
      "Train on 478 samples, validate on 478 samples\n",
      "478/478 [==============================] - 1s 3ms/sample - loss: 0.0309 - val_loss: 0.0371\n",
      "\n",
      "Epoch 9.\n",
      "Train on 46501 samples, validate on 9617 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46501/46501 [==============================] - 38s 810us/sample - loss: 0.0430 - val_loss: 0.0443\n",
      "Train on 36884 samples, validate on 9165 samples\n",
      "36884/36884 [==============================] - 42s 1ms/sample - loss: 0.0425 - val_loss: 0.0447\n",
      "Train on 27719 samples, validate on 8549 samples\n",
      "27719/27719 [==============================] - 42s 2ms/sample - loss: 0.0409 - val_loss: 0.0433\n",
      "Train on 19170 samples, validate on 7703 samples\n",
      "19170/19170 [==============================] - 34s 2ms/sample - loss: 0.0391 - val_loss: 0.0412\n",
      "Train on 11467 samples, validate on 6149 samples\n",
      "11467/11467 [==============================] - 25s 2ms/sample - loss: 0.0382 - val_loss: 0.0361\n",
      "Train on 5318 samples, validate on 4840 samples\n",
      "5318/5318 [==============================] - 15s 3ms/sample - loss: 0.0333 - val_loss: 0.0470\n",
      "Train on 478 samples, validate on 478 samples\n",
      "478/478 [==============================] - 2s 3ms/sample - loss: 0.0299 - val_loss: 0.0355\n",
      "\n",
      "Epoch 10.\n",
      "Train on 46501 samples, validate on 9617 samples\n",
      "46501/46501 [==============================] - 37s 804us/sample - loss: 0.0427 - val_loss: 0.0450\n",
      "Train on 36884 samples, validate on 9165 samples\n",
      "36884/36884 [==============================] - 41s 1ms/sample - loss: 0.0420 - val_loss: 0.0448\n",
      "Train on 27719 samples, validate on 8549 samples\n",
      "27719/27719 [==============================] - 40s 1ms/sample - loss: 0.0401 - val_loss: 0.0432\n",
      "Train on 19170 samples, validate on 7703 samples\n",
      "19170/19170 [==============================] - 34s 2ms/sample - loss: 0.0381 - val_loss: 0.0404\n",
      "Train on 11467 samples, validate on 6149 samples\n",
      "11467/11467 [==============================] - 25s 2ms/sample - loss: 0.0372 - val_loss: 0.0350\n",
      "Train on 5318 samples, validate on 4840 samples\n",
      "5318/5318 [==============================] - 14s 3ms/sample - loss: 0.0323 - val_loss: 0.0460\n",
      "Train on 478 samples, validate on 478 samples\n",
      "478/478 [==============================] - 1s 3ms/sample - loss: 0.0283 - val_loss: 0.0346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}.')\n",
    "    \n",
    "    for sequence_length in x_train:\n",
    "        model.fit(\n",
    "            x_train[sequence_length],\n",
    "            y_train[sequence_length],\n",
    "            verbose=1,\n",
    "            validation_data=(x_test[sequence_length], y_test[sequence_length]),\n",
    "        )\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_test, y_pred):\n",
    "    f1s = list()\n",
    "    \n",
    "    for test, pred in zip(y_test, y_pred):\n",
    "        test = test[-1]\n",
    "        pred = pred[-1]\n",
    "        \n",
    "        f1_ = f1_score(test, (pred > 0.5).astype(int))\n",
    "        \n",
    "        f1s.append(f1_)\n",
    "    \n",
    "    return np.mean(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_len\tf1\n",
      "1\t0.4981297462904875\n",
      "2\t0.5591226013089906\n",
      "3\t0.581323399599347\n",
      "4\t0.5971323636095175\n",
      "5\t0.6186631981501756\n",
      "6\t0.6379651289411066\n",
      "7\t0.6406057093879871\n"
     ]
    }
   ],
   "source": [
    "print('seq_len\\tf1')\n",
    "\n",
    "for sequence_length in x_test:\n",
    "    y_pred = model.predict(x_test[sequence_length])\n",
    "    \n",
    "    print(f'{sequence_length}\\t{f1(y_test[sequence_length], y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Топ-5 предсказанных категорий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlargest_ids(array, top=5):\n",
    "    return array.argsort()[:-top - 1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_items = np.array(unique_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer id: 874e4643199a9adcd65e85d733679c33\n",
      "real: [5411 5541 5661 5812 5814]\n",
      "pred: [5411 5541 5812 5814 6011]\n",
      "\n",
      "customer id: 27dcbe0058941b092c03e05a360d55c5\n",
      "real: [5074 5085 5411 5541 6011]\n",
      "pred: [5411 5541 5814 5921 6011]\n",
      "\n",
      "customer id: aa51500ce03cca814cc5f9f02551b242\n",
      "real: [5198 5331 5411 5814 9402]\n",
      "pred: [5411 5499 5541 5912 6011]\n",
      "\n",
      "customer id: 1f6659c9a9cd28fc8e0f484ddf56d1cd\n",
      "real: [5310 5311 5399 5411 5499]\n",
      "pred: [5411 5812 5814 5912 6011]\n",
      "\n",
      "customer id: ae18d7f44678a0f682575699e006162a\n",
      "real: [5310 5311 5411 5691 6011]\n",
      "pred: [5411 5499 5691 5912 6011]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for customer_id in np.random.choice(unique_customers, 5, replace=False):\n",
    "    customer_baskets = encoded_baskets[data['customer_id'] == customer_id]\n",
    "\n",
    "    next_basket_real = customer_baskets[-1]\n",
    "\n",
    "    top_5_ids_real = get_nlargest_ids(next_basket_real)\n",
    "    top_5_results_real = unique_items[top_5_ids_real]\n",
    "\n",
    "    customer_baskets = customer_baskets[:-1]\n",
    "    customer_baskets = customer_baskets[np.newaxis, :]\n",
    "    \n",
    "    next_baskets_pred = model.predict(customer_baskets)[0]\n",
    "    next_basket_pred = next_baskets_pred[-1]\n",
    "\n",
    "    top_5_ids_pred = get_nlargest_ids(next_basket_pred)\n",
    "    top_5_results_pred = unique_items[top_5_ids_pred]\n",
    "    \n",
    "    top_5_results_real.sort()\n",
    "    top_5_results_pred.sort()\n",
    "    \n",
    "    print(f'customer id: {customer_id}')\n",
    "    \n",
    "    print(f'real: {top_5_results_real}')\n",
    "    print(f'pred: {top_5_results_pred}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
